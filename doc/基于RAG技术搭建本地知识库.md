# 什么是 RAG

大型语言模型（LLMs）所实现的最强大应用之一是复杂的问答（Q&A）聊天机器人。这些应用能够回答关于特定源信息的问题。这些应用使用一种被称为检索增强生成（Retrieval Augmented Generation，简称 RAG）的技术。

RAG 是一种用额外数据增强 LLM 知识的技术。

LLM 可以对广泛的主题进行推理，但它们的知识仅限于训练时截止日期之前的公开数据。如果你想构建能够对私有数据或模型截止日期之后引入的数据进行推理的 AI 应用，你需要用它所需的特定信息来增强模型的知识。将适当的信息引入并插入到模型提示中的过程被称为检索增强生成（RAG）。

## 实现 RAG 的一般步骤

一个典型的 RAG 应用通常包含两个主要部分：

- 索引：从数据源摄取数据并建立索引。这通常在离线状态下进行。
- 检索和生成：实际的 RAG 链，它在运行时接收用户查询，从索引中检索相关数据，然后将其传递给模型。

## 建立 Indexing

- Load：首先我们需要加载我们的数据。这通过文档加载器来完成。
- Split：文本分割器将大型文档分割成更小的块。这对于索引数据和将其传递给模型都很有用，因为大块数据更难搜索，而且无法适应模型有限的上下文窗口。
- Embed：嵌入模型将文本转换为向量表示。这些向量用于相似性搜索和检索。
- Store：我们需要一个地方来存储和索引我们的分割，以便之后可以对其进行搜索。这通常使用向量存储和嵌入模型来完成。

![](http://192.168.99.63:3000/uploads/upload_c5d765601537257c91519e4b931a5640.png)

## 检索和生成(Retrieval and generation)

- Retrieve: 给定一个用户输入，使用 Retriever 从存储中检索相关分割。
- Generate: 一个 ChatModel / LLM 使用一个包含问题和检索数据的提示来生成答案。

![](http://192.168.99.63:3000/uploads/upload_4fd1820c46b9d23efdaa0bc2755a9042.png)

# 如何建立一个高质量的 RAG 系统

建立高质量的 RAG 系统对于提高 AI 应用的性能和用户体验至关重要。以下是几个主要原因：

1. 提高回答准确性：高质量的 RAG 系统能够更精准地检索相关信息，为语言模型提供更准确的上下文，从而生成更准确的回答。

2. 增强知识覆盖范围：通过整合外部知识库，RAG 系统可以弥补语言模型在特定领域或最新信息方面的不足，大大扩展了 AI 应用的知识范围。

3. 减少幻觉：优质的 RAG 系统能够提供可靠的信息源，有效降低语言模型产生虚假或不准确信息的可能性。

4. 提升响应速度：高效的检索和处理机制可以缩短系统响应时间，提供更快速的用户体验。

5. 增强可解释性：通过提供信息来源，RAG 系统使 AI 的回答更具可追溯性和可解释性，增加了用户对系统的信任度。

6. 灵活性和可扩展性：高质量的 RAG 系统允许轻松更新和扩展知识库，使 AI 应用能够持续适应新的信息和变化的需求。

因此，构建高质量的 RAG 系统是提升 AI 应用整体性能和用户满意度的关键。接下来，我们将详细探讨如何实现这一目标。

## 1. 选择合适的文档加载器

选择合适的文档加载器是构建高质量 RAG 系统的第一步。文档加载器负责从各种数据源中提取文本内容。以下是一些常见的文档加载器类型(参考[LangChain-文档加载器](https://python.langchain.com/docs/integrations/document_loaders/))：

- 文本文件加载器：用于加载纯文本文件(.txt，.md)
- PDF 加载器：用于加载 PDF 文档
- Word 文档加载器：用于加载 Microsoft Word 文档(.doc, .docx)
- HTML 加载器：用于加载网页内容
- CSV/Excel 加载器：用于加载结构化数据
- 数据库加载器：用于从数据库中提取文本数据

选择加载器时，需要考虑以下因素：

1. 数据源格式：确保选择的加载器能够正确处理你的数据格式。
2. 元数据提取：某些加载器可以提取额外的元数据（如作者、日期等），这对后续处理可能有用。
3. 性能：对于大规模数据，选择高效的加载器很重要。
4. 容错能力：好的加载器应该能够处理常见的文档错误和异常情况。

## 2. 优化文本分割策略

文档分割通常是许多应用程序的关键预处理步骤。它涉及将大型文本分解成更小、更易管理的块。这个过程提供了几个好处，例如确保对不同长度的文档进行一致的处理、克服模型的输入大小限制，以及提高检索系统中使用的文本表示的质量。有几种分割文档的策略，每种策略都有其自身的优势。

### 为什么要对文档进行分割？

- 处理非均匀文档长度：现实世界的文档集合通常包含不同大小的文本。分割确保了对所有文档的一致处理。
- 克服模型限制：许多嵌入模型和语言模型都有最大输入大小限制。分割允许我们处理原本会超出这些限制的文档。
- 提高表示质量：对于较长的文档，嵌入或其他表示的质量可能会下降，因为它们试图捕捉太多信息。分割可以导致每个部分更加集中和准确的表示。
- 增强检索精度：在信息检索系统中，分割可以提高搜索结果的粒度，允许查询与相关文档部分更精确地匹配。
- 优化计算资源：处理较小的文本块可以更加内存高效，并允许更好地并行化处理任务。

### 该怎么分割文档？

#### 基于长度(Length-based):

最直观的策略是根据文档长度进行分割。这种简单而有效的方法确保每个块不超过指定的大小限制。基于长度分割的主要优点：

- 实现简单直接
- 块大小一致
- 易于适应不同的模型要求

基于长度分割的类型：

- 基于标记（Token）：根据标记数量分割文本，这在使用语言模型时很有用。
- 基于字符：根据字符数量分割文本，这可以在不同类型的文本中保持更一致。

#### 基于文本结构(Text-structured):

文本自然地组织成层次结构单元，如段落、句子和单词。我们可以利用这种固有结构来指导我们的分割策略，创建能够保持自然语言流动、维持分割内部语义连贯性，并适应不同级别文本粒度的分割。LangChain 的 RecursiveCharacterTextSplitter 实现了这一概念：

- RecursiveCharacterTextSplitter 尝试保持较大的单元（如段落）完整。
- 如果一个单元超过了块大小，它会移动到下一级（如句子）。
- 如果需要，这个过程会一直持续到单词级别。

#### 基于文档结构(Document-structured):

一些文档具有固有的结构，例如 HTML、Markdown 或 JSON 文件。在这些情况下，根据文档结构进行分割是有益的，因为它通常自然地将语义相关的文本分组。基于结构的分割的主要优势：

- 保留文档的逻辑组织
- 在每个块内保持上下文
- 对于下游任务（如检索或摘要）可能更有效
  基于结构的分割的例子：

基于结构的分割的例子：

- Markdown：基于标题进行分割（例如，#、##、###）
- HTML：使用标签进行分割
- JSON：按对象或数组元素进行分割
- 代码：按函数、类或逻辑块进行分割

#### 基于语义(Semantic meaning-based):

[semantic-chunker（一种基于语义的分割策略）](https://python.langchain.com/docs/how_to/semantic-chunker/)

与之前的方法不同，基于语义的分割实际上考虑了文本的内容。虽然其他方法使用文档或文本结构作为语义含义的代理，但这种方法直接分析文本的语义。有几种方法可以实现这一点，但从概念上讲，当文本含义发生显著变化时，这种方法会分割文本。例如，我们可以使用滑动窗口方法生成嵌入，并比较嵌入以找出显著差异：

- 从前几个句子开始，生成一个嵌入。
- 移动到下一组句子，生成另一个嵌入（例如，使用滑动窗口方法）。
- 比较嵌入以找出显著差异，这表明语义部分之间可能存在"断点"。

这种技术有助于创建语义上更连贯的块，可能会提高检索或摘要等下游任务的质量。

## 3. 选择合适的嵌入模型

选择合适的嵌入模型对于构建高效的 RAG 系统至关重要。嵌入模型将文本转换为向量表示，这些向量用于相似性搜索和检索。以下是选择嵌入模型时需要考虑的几个关键因素：

1. 模型性能：

   - 选择在相关任务和数据集上表现良好的模型。
   - 考虑模型在语义相似度、文本分类等任务上的评估结果。

2. 向量维度：

   - 较高维度的向量通常能捕捉更多语义信息，但也会增加存储和计算开销。
   - 根据具体应用需求和资源限制选择合适的维度。

3. 计算效率：

   - 考虑模型的推理速度，特别是在处理大规模数据时。
   - 评估模型在目标硬件上的性能表现。

4. 领域适应性：

   - 选择在目标领域或相似领域上预训练的模型。
   - 考虑是否需要进行领域适应性微调。

5. 多语言支持：

   - 如果系统需要处理多语言内容，选择支持多语言的嵌入模型。

6. 模型大小：

   - 考虑部署环境的资源限制，选择适合的模型大小。
   - 权衡模型大小和性能之间的关系。

7. 更新频率：

   - 考虑模型的更新频率，以确保能够利用最新的改进。

8. 社区支持：

   - 选择有活跃社区支持的模型，便于获取帮助和资源。

9. 许可证：
   - 确保模型的使用许可符合你的项目需求。

一些常用的嵌入模型选择：

- Sentence-BERT (SBERT)：专为语义相似度任务优化的 BERT 变体。
- OpenAI 的 text-embedding-ada-002：性能强大的通用嵌入模型。
- HuggingFace 的 all-MiniLM-L6-v2：轻量级模型，适合资源受限的环境。
- FastText：Facebook 开发的快速文本表示学习模型。
- Universal Sentence Encoder：Google 开发的通用句子编码器。

选择嵌入模型后，还需要考虑如何有效地存储和检索这些向量。常用的向量数据库或相似性搜索引擎包括：

- Faiss：Facebook AI 开发的高效相似性搜索库。
- Annoy：Spotify 开发的近似最近邻搜索库。
- Elasticsearch 与 vector search 插件：结合全文搜索和向量搜索的能力。
- Pinecone：专门为向量搜索优化的云服务。
